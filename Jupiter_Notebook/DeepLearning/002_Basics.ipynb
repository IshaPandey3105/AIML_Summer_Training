{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c394b3",
   "metadata": {},
   "source": [
    "# 🎯 Deep Learning Notes – Topics 1 to 3\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 1. Introduction to Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 What is Deep Learning?\n",
    "\n",
    "Deep Learning (DL) is a part of Machine Learning (ML) that uses **neural networks with many layers** to learn from large volumes of data.\n",
    "\n",
    "🧠 Inspired by the structure of the human brain, DL networks can:\n",
    "- Automatically extract features\n",
    "- Learn complex patterns\n",
    "- Handle unstructured data (images, text, audio)\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 ML vs DL\n",
    "\n",
    "| Feature                | Machine Learning            | Deep Learning                    |\n",
    "|------------------------|-----------------------------|----------------------------------|\n",
    "| Feature Engineering    | Manual (you design features)| Automatic (learns from raw data) |\n",
    "| Performance on big data| May plateau                 | Improves with more data          |\n",
    "| Data Requirement       | Can work on small data      | Needs large data sets            |\n",
    "| Examples               | Decision Trees, SVM         | CNNs, RNNs, Transformers         |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧭 When to Use DL?\n",
    "\n",
    "✅ Use Deep Learning when:\n",
    "- You have **lots of labeled data**\n",
    "- The problem involves **images, audio, or text**\n",
    "- You want **automatic feature extraction**\n",
    "- You need **state-of-the-art accuracy**\n",
    "\n",
    "📌 Not ideal for small tabular datasets with limited data — traditional ML might perform better there.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 2. Neural Networks Fundamentals\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 Structure of Neural Networks\n",
    "\n",
    "A neural network consists of:\n",
    "\n",
    "1. **Input Layer** – takes the input features (e.g., pixel values, words).\n",
    "2. **Hidden Layers** – multiple layers of neurons doing computation.\n",
    "3. **Output Layer** – produces final prediction (e.g., class label)\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 Each Neuron Does:\n",
    "```\n",
    "Z = W·X + b  \n",
    "A = Activation(Z)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- W = weight  \n",
    "- X = input  \n",
    "- b = bias  \n",
    "- Z = linear sum  \n",
    "- A = activated output\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Forward Propagation\n",
    "\n",
    "This is the **process of calculating outputs** from the input, layer by layer.\n",
    "\n",
    "📌 In each neuron:\n",
    "- Multiply input with weights\n",
    "- Add bias\n",
    "- Pass through activation function\n",
    "- Send output to next layer\n",
    "\n",
    "#### 🧮 Example:\n",
    "Given:\n",
    "- Input: X = 1.0  \n",
    "- Weight: W = 2.0  \n",
    "- Bias: b = 1.0\n",
    "\n",
    "Then:\n",
    "```\n",
    "Z = W·X + b = 2.0 * 1.0 + 1.0 = 3.0  \n",
    "A = ReLU(Z) = max(0, 3.0) = 3.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 Loss Function & Cost Function\n",
    "\n",
    "- **Loss** = error for one training example  \n",
    "- **Cost** = average error over all training examples\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Common Loss Functions:\n",
    "\n",
    "**Mean Squared Error (MSE)** – for regression:\n",
    "```\n",
    "L = (1/n) * Σ (y_pred - y_true)^2\n",
    "```\n",
    "\n",
    "**Cross-Entropy Loss** – for classification:\n",
    "```\n",
    "L = - Σ y_true * log(y_pred)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Backpropagation & Gradient Descent\n",
    "\n",
    "This is how the network **learns** by adjusting weights and biases.\n",
    "\n",
    "Steps:\n",
    "1. Calculate prediction error (loss)\n",
    "2. Compute gradient of the loss (slope)\n",
    "3. Update weights using gradient descent\n",
    "\n",
    "#### 🎯 Gradient Descent Formula:\n",
    "```\n",
    "W = W - α * ∂L/∂W\n",
    "```\n",
    "Where:\n",
    "- α = learning rate (how big a step to take)\n",
    "- ∂L/∂W = gradient (slope of loss w.r.t. weights)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 3. Activation Functions\n",
    "\n",
    "---\n",
    "\n",
    "Activation functions introduce **non-linearity**, allowing the network to learn complex patterns. Without them, the network would be just a linear model.\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ Sigmoid\n",
    "\n",
    "#### 📌 Formula:\n",
    "```\n",
    "A = 1 / (1 + e^(-Z))\n",
    "```\n",
    "\n",
    "- Output range: (0, 1)\n",
    "- Use: Output layer in binary classification\n",
    "\n",
    "#### Example:\n",
    "Z = 2 → A ≈ 0.88  \n",
    "Z = -2 → A ≈ 0.12\n",
    "\n",
    "🧨 Downside: Vanishing gradients\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Tanh (Hyperbolic Tangent)\n",
    "\n",
    "#### 📌 Formula:\n",
    "```\n",
    "A = (e^Z - e^(-Z)) / (e^Z + e^(-Z))\n",
    "```\n",
    "\n",
    "- Output range: (-1, 1)\n",
    "- Use: Hidden layers (better than sigmoid for centered data)\n",
    "\n",
    "#### Example:\n",
    "Z = 1 → A ≈ 0.76  \n",
    "Z = -1 → A ≈ -0.76\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ ReLU (Rectified Linear Unit)\n",
    "\n",
    "#### 📌 Formula:\n",
    "```\n",
    "A = max(0, Z)\n",
    "```\n",
    "\n",
    "- Output range: [0, ∞)\n",
    "- Very popular in hidden layers\n",
    "\n",
    "#### Example:\n",
    "Z = -3 → A = 0  \n",
    "Z = 4 → A = 4\n",
    "\n",
    "⚠️ Can cause \"dying neurons\" if too many 0s\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Leaky ReLU\n",
    "\n",
    "#### 📌 Formula:\n",
    "```\n",
    "A = Z if Z > 0 else 0.01 * Z\n",
    "```\n",
    "\n",
    "- Fixes ReLU's zero output for negative Z\n",
    "- Range: (-∞, ∞)\n",
    "\n",
    "#### Example:\n",
    "Z = -2 → A = -0.02\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ Softmax\n",
    "\n",
    "#### 📌 Formula:\n",
    "```\n",
    "A[i] = exp(Z[i]) / Σ exp(Z[j])\n",
    "```\n",
    "\n",
    "- Converts outputs into probabilities\n",
    "- Use: Output layer for multi-class classification\n",
    "\n",
    "#### Example:\n",
    "Z = [1, 2, 3]  \n",
    "Softmax(Z) ≈ [0.09, 0.24, 0.67]\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Summary: When to Use Which?\n",
    "\n",
    "| Activation     | Where to Use                         |\n",
    "|----------------|--------------------------------------|\n",
    "| **Sigmoid**    | Output layer in binary classification|\n",
    "| **Tanh**       | Hidden layers with centered data     |\n",
    "| **ReLU**       | Most common in hidden layers         |\n",
    "| **Leaky ReLU** | Hidden layers (to fix dead neurons)  |\n",
    "| **Softmax**    | Output layer in multi-class tasks    |\n",
    "\n",
    "---\n",
    "\n",
    "🎉 Congratulations! You’ve now learned:\n",
    "\n",
    "- ✅ What Deep Learning is  \n",
    "- ✅ How neural networks work  \n",
    "- ✅ How data flows through layers  \n",
    "- ✅ What activation functions do & when to use them\n",
    "\n",
    "🧪 Next: Training models with Keras + real projects like digit recognition!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f5e57",
   "metadata": {},
   "source": [
    "# ✅ Topic 4: Training Deep Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Epochs, Batch Size & Iterations\n",
    "\n",
    "When training a deep neural network, data is not fed all at once — it's split into smaller parts:\n",
    "\n",
    "### 🔹 Epoch\n",
    "- One full pass of the training dataset through the model.\n",
    "- If you have 1,000 samples and train for 10 epochs → model sees each sample 10 times.\n",
    "\n",
    "### 🔹 Batch Size\n",
    "- Number of training examples used in one forward/backward pass.\n",
    "- Smaller batch = faster updates but more noise.\n",
    "- Common values: 16, 32, 64, 128\n",
    "\n",
    "### 🔹 Iteration\n",
    "- One update of weights.\n",
    "- No. of iterations per epoch = Total samples / Batch size\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Example:\n",
    "Dataset = 1,000 images  \n",
    "Batch size = 100  \n",
    "Epochs = 3\n",
    "\n",
    "Then:\n",
    "- 1 Epoch = 10 Iterations (1000 / 100)\n",
    "- 3 Epochs = 30 Iterations in total\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Overfitting vs Underfitting\n",
    "\n",
    "| Type          | Description                                  | Symptoms                      | Fix                          |\n",
    "|---------------|----------------------------------------------|-------------------------------|------------------------------|\n",
    "| Underfitting  | Model is too simple, can't capture patterns  | High training & test error    | Add layers, train longer     |\n",
    "| Overfitting   | Model memorizes training data                | Low train error, high test error | Regularization, dropout, early stopping |\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Visual Representation:\n",
    "\n",
    "- **Underfitting**: Both training & validation loss are high  \n",
    "- **Good Fit**: Both losses are low  \n",
    "- **Overfitting**: Training loss ↓ but validation loss ↑\n",
    "\n",
    "---\n",
    "\n",
    "## 🛡️ Regularization Techniques\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 1. Dropout\n",
    "- Randomly turns off neurons during training to avoid co-dependency.\n",
    "- Forces the model to learn robust features.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Dropout\n",
    "model.add(Dropout(0.5))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. L1 & L2 Regularization\n",
    "\n",
    "- Adds penalty to the loss function to discourage large weights.\n",
    "\n",
    "#### 🔸 L1 (Lasso):\n",
    "- Promotes sparsity (more zeros in weights)\n",
    "\n",
    "#### 🔸 L2 (Ridge):\n",
    "- Penalizes large weights smoothly\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Training Workflow Overview\n",
    "\n",
    "1. Define model architecture  \n",
    "2. Compile model (optimizer + loss + metrics)  \n",
    "3. Train using `.fit()` with training data  \n",
    "4. Validate on unseen data  \n",
    "5. Tune epochs, batch size, and layers  \n",
    "6. Prevent overfitting using dropout/regularization\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Optimizer Quick Intro (Preview of Topic 5)\n",
    "\n",
    "| Optimizer | Description                  | Notes                    |\n",
    "|----------|------------------------------|--------------------------|\n",
    "| SGD      | Vanilla Stochastic Gradient  | Basic, slower            |\n",
    "| Adam     | Adaptive + momentum          | Most commonly used       |\n",
    "| RMSProp  | Scales learning rate adaptively | Good for RNNs           |\n",
    "\n",
    "---\n",
    "\n",
    "🎯 In the next lesson:\n",
    "We’ll **build and train** your first neural network using **Keras** on the **MNIST digit dataset**.\n",
    "\n",
    "You'll learn:\n",
    "- How to load data\n",
    "- Build architecture\n",
    "- Train, evaluate, and visualize results!\n",
    "\n",
    "Ready to code some neurons? 🧠💻\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
